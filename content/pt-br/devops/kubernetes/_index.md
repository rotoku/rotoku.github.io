---
title: "Kubernetes"
linkTitle: "Kubernetes"
date: 2023-10-07
weight: 10
---

---

---

---

# Kubernetes

## Kinds

| kind                  |       version        |
| :-------------------- | :------------------: |
| Pod                   |          v1          |
| Service               |          v1          |
| ReplicaSet            |       apps/v1        |
| Deployment            |       apps/v1        |
| Namespace             |          v1          |
| PersistentVolume      |          v1          |
| PersistentVolumeClaim |          v1          |
| StorageClass          |  storage.k8s.io/v1   |
| StatefullSet          |       apps/v1        |
| Secret                |          v1          |
| ConfigMap             |          v1          |
| Ingress               | networking.k8s.io/v1 |

## Entendendo o que sÃ£o containers e o kubernetes

### o que Ã© container?

Containers sÃ£o unidades executÃ¡veis de software em que o cÃ³digo do aplicativo Ã© empacotado com suas respectivas bibliotecas e dependÃªncias, usando mÃ©todos comuns para executÃ¡-los em qualquer lugar, seja em um computador desktop, na estrutura de TI tradicional ou na cloud.
Ã‰ um isolamento de recursos.
o que pode ser recursos:

- cpu
- memÃ³ria
- disco

- Namespaces: namespaces disponibilizam um mecanismo para isolar grupos de recursos dentro de um Ãºnico cluster.
- chroot: O comando chroot do sistema operacional Unix Ã© uma operaÃ§Ã£o que muda o diretÃ³rio root do processo corrente e de seus processos filhos. Um programa que Ã© re-rooted para um outro diretÃ³rio nÃ£o pode acessar arquivos fora daquele diretÃ³rio, e o diretÃ³rio Ã© chamado de "prisÃ£o chroot"
- Containers:
  - docker
  - podman
  - CRI-O

### o que Ã© container engine?

- nÃ£o aceita mais docker como container engine, por que havia uma gambi chamado Dockershim que foi removido do Kubernetes na release v1.24
- Ã© o cara responsÃ¡vel pela integraÃ§Ã£o com os recursos
- podem executar vÃ¡rias instÃ¢ncias isoladas, conhecidas como contÃªineres, no mesmo kernel do sistema operacional . Os contÃªineres executam a virtualizaÃ§Ã£o no nÃ­vel do sistema operacional e fornecem um ambiente controlÃ¡vel e facilmente gerenciÃ¡vel para a execuÃ§Ã£o de aplicativos e dependÃªncias.

### o que Ã© um container runtime?

- conversa com o kernel
- responsÃ¡vel por executar o container
- cgroups/containers
- Tipos:
  1. alto nÃ­vel: fala com o kernel (containerd, o CRI-O e o Podman.)
  2. baixo nÃ­vel: fala com as aplicaÃ§Ãµes (runc, o crun e o runsc.)

### o que Ã© a OCI?

A Open Container Initiative Ã© um projeto da Linux Foundation, iniciado em junho de 2015 por Docker, CoreOS e os mantenedores do appc para projetar padrÃµes abertos para virtualizaÃ§Ã£o em nÃ­vel de sistema operacional.

### O que Ã© o kubernetes?

Como Kubernetes Ã© uma palavra difÃ­cil de se pronunciar - e de se escrever - a comunidade simplesmente o apelidou de k8s, seguindo o padrÃ£o i18n (a letra "k" seguida por oito letras e o "s" no final), pronunciando-se simplesmente "kates".

- em 2014 a google criou o projeto BORG, desenvolvido em go
  Ã‰ um orquestrador de Containers. - subir mais containers, baixar - outros exemplos de orquestradores: nomad, ecs, mesos, swarm

### O que sÃ£o os workers e o control plane do kubernetes?

![Arquitetura](../../imgs/arquitetura_k8s.jpeg)

### Quais os componentes do control plane do kubernetes?

![alt text](../../imgs/componentes_control_plane.jpeg)

### Quais os componentes dos workers do kubernetes?

![alt text](../../imgs/componentes_workers.jpeg)

### Todos os componentes

- API Server: Ã‰ um dos principais componentes do k8s. Este componente fornece uma API que utiliza JSON sobre HTTP para comunicaÃ§Ã£o, onde para isto Ã© utilizado principalmente o utilitÃ¡rio kubectl, por parte dos administradores, para a comunicaÃ§Ã£o com os demais nÃ³s, como mostrado no grÃ¡fico. Estas comunicaÃ§Ãµes entre componentes sÃ£o estabelecidas atravÃ©s de requisiÃ§Ãµes REST;

- etcd: O etcd Ã© um datastore chave-valor distribuÃ­do que o k8s utiliza para armazenar as especificaÃ§Ãµes, status e configuraÃ§Ãµes do cluster. Todos os dados armazenados dentro do etcd sÃ£o manipulados apenas atravÃ©s da API. Por questÃµes de seguranÃ§a, o etcd Ã© por padrÃ£o executado apenas em nÃ³s classificados como control plane no cluster k8s, mas tambÃ©m podem ser executados em clusters externos, especÃ­ficos para o etcd, por exemplo;

- Scheduler: O scheduler Ã© responsÃ¡vel por selecionar o nÃ³ que irÃ¡ hospedar um determinado pod (a menor unidade de um cluster k8s - nÃ£o se preocupe sobre isso por enquanto, nÃ³s falaremos mais sobre isso mais tarde) para ser executado. Esta seleÃ§Ã£o Ã© feita baseando-se na quantidade de recursos disponÃ­veis em cada nÃ³, como tambÃ©m no estado de cada um dos nÃ³s do cluster, garantindo assim que os recursos sejam bem distribuÃ­dos. AlÃ©m disso, a seleÃ§Ã£o dos nÃ³s, na qual um ou mais pods serÃ£o executados, tambÃ©m pode levar em consideraÃ§Ã£o polÃ­ticas definidas pelo usuÃ¡rio, tais como afinidade, localizaÃ§Ã£o dos dados a serem lidos pelas aplicaÃ§Ãµes, etc;

- Controller Manager: Ã‰ o controller manager quem garante que o cluster esteja no Ãºltimo estado definido no etcd. Por exemplo: se no etcd um deploy estÃ¡ configurado para possuir dez rÃ©plicas de um pod, Ã© o controller manager quem irÃ¡ verificar se o estado atual do cluster corresponde a este estado e, em caso negativo, procurarÃ¡ conciliar ambos;

- Kubelet: O kubelet pode ser visto como o agente do k8s que Ã© executado nos nÃ³s workers. Em cada nÃ³ worker deverÃ¡ existir um agente Kubelet em execuÃ§Ã£o. O Kubelet Ã© responsÃ¡vel por de fato gerenciar os pods, que foram direcionados pelo controller do cluster, dentro dos nÃ³s, de forma que para isto o Kubelet pode iniciar, parar e manter os contÃªineres e os pods em funcionamento de acordo com o instruÃ­do pelo controlador do cluster;

- Kube-proxy: Age como um proxy e um load balancer. Este componente Ã© responsÃ¡vel por efetuar roteamento de requisiÃ§Ãµes para os pods corretos, como tambÃ©m por cuidar da parte de rede do nÃ³;

- Pod: Ã‰ o menor objeto do k8s. Como dito anteriormente, o k8s nÃ£o trabalha com os contÃªineres diretamente, mas organiza-os dentro de pods, que sÃ£o abstraÃ§Ãµes que dividem os mesmos recursos, como endereÃ§os, volumes, ciclos de CPU e memÃ³ria. Um pod pode possuir vÃ¡rios contÃªineres;

- Deployment: Ã‰ um dos principais controllers utilizados. O Deployment, em conjunto com o ReplicaSet, garante que determinado nÃºmero de rÃ©plicas de um pod esteja em execuÃ§Ã£o nos nÃ³s workers do cluster. AlÃ©m disso, o Deployment tambÃ©m Ã© responsÃ¡vel por gerenciar o ciclo de vida das aplicaÃ§Ãµes, onde caracterÃ­sticas associadas a aplicaÃ§Ã£o, tais como imagem, porta, volumes e variÃ¡veis de ambiente, podem ser especificados em arquivos do tipo yaml ou json para posteriormente serem passados como parÃ¢metro para o kubectl executar o deployment. Esta aÃ§Ã£o pode ser executada tanto para criaÃ§Ã£o quanto para atualizaÃ§Ã£o e remoÃ§Ã£o do deployment;

- ReplicaSets: Ã‰ um objeto responsÃ¡vel por garantir a quantidade de pods em execuÃ§Ã£o no nÃ³;

- Services: Ã‰ uma forma de vocÃª expor a comunicaÃ§Ã£o atravÃ©s de um ClusterIP, NodePort ou LoadBalancer para distribuir as requisiÃ§Ãµes entre os diversos Pods daquele Deployment. Funciona como um balanceador de carga.

### Quais as portas TCP e UDP dos componentes do kubernetes?

#### Control Plane

- kube-api server: 6443 tcp
- etcd: 2379,2380 tcp
- kubelet: 10250 tcp
- kube-scheduler: 10251 tcp
- kube-controller: 10252 tcp

#### WORKERS

- kubelet: 10250 tcp
- nodeport: 30000-32767 tcp

- weave net: 6783-6784 tcp/udp

### IntroduÃ§Ã£o a pods, replica sets, deployments e service

- menor unidade de gerenciamento do k8s Ã© o POD
- um pod pode ter mais de um container
  ![alt text](../../imgs/pods_replicasets_deployments_service.jpeg)

### Entendendo e instalando o kubectl

```
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl
kubectl version --client --short
```

#### docker

```
curl -fsSL https://get.docker.com | bash
```

#### [kind](https://kind.sigs.k8s.io/docs/user/quick-start/)

```
# For AMD64 / x86_64
[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.25.0/kind-linux-amd64
# For ARM64
[ $(uname -m) = aarch64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.25.0/kind-linux-arm64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
```

```
kind create cluster
kubectl get nodes
kind delete cluster --name $(kind get clusters)
```

criando cluster com 3 nodes

```cluster.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
```

```
kind create cluster --config /media/rkumabe/DATA/git/rotoku.github.io/content/pt-br/devops/kubernetes/files/01/meu-primeiro-cluster.yaml --name giropops
```

### Primeiros passos no kubernetes com o kubectl

```
kubectl get nodes
kubectl get pods
kubectl get deployments
kubectl get namespaces
kubectl get service
kubectl get replicaset
kubectl get pods -n kube-system
kubectl get pods -n kube-system -o wide
## Pegar pods de todos namespaces
kubectl get pods -A
kubectl run --image nginx --port=80 giropops
kubectl get pods
kubectl exec -it giropops -- bash
kubectl delete pods giropops
kubectl get pods
kubectl run --image nginx --port=80 nginx --dry-run=client -o yaml > nginx.yaml
kubectl apply -f /media/rkumabe/DATA/git/rotoku.github.io/content/pt-br/devops/kubernetes/files/01/meu-primeiro-pod.yaml
```

### Criando um cluster Kubernetes

### Criando o cluster em sua mÃ¡quina local

Vamos mostrar algumas opÃ§Ãµes, caso vocÃª queira comeÃ§ar a brincar com o Kubernetes utilizando somente a sua mÃ¡quina local, o seu desktop.

Lembre-se, vocÃª nÃ£o Ã© obrigado a testar/utilizar todas as opÃ§Ãµes abaixo, mas seria muito bom caso vocÃª testasse. :D

#### Minikube

##### Requisitos bÃ¡sicos

Ã‰ importante frisar que o Minikube deve ser instalado localmente, e nÃ£o em um _cloud provider_. Por isso, as especificaÃ§Ãµes de _hardware_ a seguir sÃ£o referentes Ã  mÃ¡quina local.

- Processamento: 1 core;
- MemÃ³ria: 2 GB;
- HD: 20 GB.

##### InstalaÃ§Ã£o do Minikube no GNU/Linux

Antes de mais nada, verifique se a sua mÃ¡quina suporta virtualizaÃ§Ã£o. No GNU/Linux, isto pode ser realizado com o seguinte comando:

```
grep -E --color 'vmx|svm' /proc/cpuinfo
```

&nbsp;
Caso a saÃ­da do comando nÃ£o seja vazia, o resultado Ã© positivo.

HÃ¡ a possibilidade de nÃ£o utilizar um _hypervisor_ para a instalaÃ§Ã£o do Minikube, executando-o ao invÃ©s disso sobre o prÃ³prio host. Iremos utilizar o Oracle VirtualBox como _hypervisor_, que pode ser encontrado [aqui](https://www.virtualbox.org).

Efetue o download e a instalaÃ§Ã£o do `Minikube` utilizando os seguintes comandos.

```
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```

&nbsp;

##### InstalaÃ§Ã£o do Minikube no MacOS

No MacOS, o comando para verificar se o processador suporta virtualizaÃ§Ã£o Ã©:

```
sysctl -a | grep -E --color 'machdep.cpu.features|VMX'
```

&nbsp;
Se vocÃª visualizar `VMX` na saÃ­da, o resultado Ã© positivo.

Efetue a instalaÃ§Ã£o do Minikube com um dos dois mÃ©todos a seguir, podendo optar-se pelo Homebrew ou pelo mÃ©todo tradicional.

```
sudo brew install minikube

minikube version
```

&nbsp;
Ou:

```
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```

&nbsp;

##### InstalaÃ§Ã£o do Minikube no Microsoft Windows

No Microsoft Windows, vocÃª deve executar o comando `systeminfo` no prompt de comando ou no terminal. Caso o retorno deste comando seja semelhante com o descrito a seguir, entÃ£o a virtualizaÃ§Ã£o Ã© suportada.

```
Hyper-V Requirements:     VM Monitor Mode Extensions: Yes
                          Virtualization Enabled In Firmware: Yes
                          Second Level Address Translation: Yes
                          Data Execution Prevention Available: Yes
```

&nbsp;
Caso a linha a seguir tambÃ©m esteja presente, nÃ£o Ã© necessÃ¡ria a instalaÃ§Ã£o de um _hypervisor_ como o Oracle VirtualBox:

```
Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
```

&nbsp;
FaÃ§a o download e a instalaÃ§Ã£o de um _hypervisor_ (preferencialmente o [Oracle VirtualBox](https://www.virtualbox.org)), caso no passo anterior nÃ£o tenha sido acusada a presenÃ§a de um. Finalmente, efetue o download do instalador do Minikube [aqui](https://github.com/kubernetes/minikube/releases/latest) e execute-o.

##### Iniciando, parando e excluindo o Minikube

Quando operando em conjunto com um _hypervisor_, o Minikube cria uma mÃ¡quina virtual, onde dentro dela estarÃ£o todos os componentes do k8s para execuÃ§Ã£o.

Ã‰ possÃ­vel selecionar qual _hypervisor_ iremos utilizar por padrÃ£o, atravÃ©s no comando abaixo:

```
minikube config set driver <SEU_HYPERVISOR>
```

&nbsp;
VocÃª deve substituir <SEU_HYPERVISOR> pelo seu hypervisor, por exemplo o KVM2, QEMU, Virtualbox ou o Hyperkit.

Caso nÃ£o queria configurar um hypervisor padrÃ£o, vocÃª pode digitar o comando `minikube start --driver=hyperkit` toda vez que criar um novo ambiente.

##### Certo, e como eu sei que estÃ¡ tudo funcionando como deveria?

Uma vez iniciado, vocÃª deve ter uma saÃ­da na tela similar Ã  seguinte:

```
minikube start --driver=virtualbox --nodes=3

ğŸ˜„  minikube v1.26.0 on Debian bookworm/sid
âœ¨  Using the qemu2 (experimental) driver based on user configuration
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸ”¥  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: default-storageclass, storage-provisioner
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

VocÃª pode entÃ£o listar os nÃ³s que fazem parte do seu _cluster_ k8s com o seguinte comando:

```
kubectl get nodes
```

&nbsp;
A saÃ­da serÃ¡ similar ao conteÃºdo a seguir:

```
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   20s   v1.25.3
```

&nbsp;
Para criar um cluster com mais de um nÃ³, vocÃª pode utilizar o comando abaixo, apenas modificando os valores para o desejado:

```
minikube start --nodes 2 -p multinode-cluster

ğŸ˜„  minikube v1.26.0 on Debian bookworm/sid
âœ¨  Automatically selected the docker driver. Other choices: kvm2, virtualbox, ssh, none, qemu2 (experimental)
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ’¾  Downloading Kubernetes v1.24.1 preload ...
    > preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 66.78 Mi
    > gcr.io/k8s-minikube/kicbase: 385.99 MiB / 386.00 MiB  100.00% 23.63 MiB p
    > gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 11s
ğŸ”¥  Creating docker container (CPUs=2, Memory=8000MB) ...
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring CNI (Container Networking Interface) ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass

ğŸ‘  Starting worker node minikube-m02 in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”¥  Creating docker container (CPUs=2, Memory=8000MB) ...
ğŸŒ  Found network options:
    â–ª NO_PROXY=192.168.11.11
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    â–ª env NO_PROXY=192.168.11.11
ğŸ”  Verifying Kubernetes components...
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

&nbsp;
Para visualizar os nÃ³s do seu novo cluster Kubernetes, digite:

```
kubectl get nodes
```

&nbsp;
Inicialmente, a intenÃ§Ã£o do Minikube Ã© executar o k8s em apenas um nÃ³, porÃ©m a partir da versÃ£o 1.10.1 Ã© possÃ­vel usar a funÃ§Ã£o de multi-node.

Caso os comandos anteriores tenham sido executados sem erro, a instalaÃ§Ã£o do Minikube terÃ¡ sido realizada com sucesso.

##### Ver detalhes sobre o cluster

```
minikube status
```

&nbsp;

##### Descobrindo o endereÃ§o do Minikube

Como dito anteriormente, o Minikube irÃ¡ criar uma mÃ¡quina virtual, assim como o ambiente para a execuÃ§Ã£o do k8s localmente. Ele tambÃ©m irÃ¡ configurar o `kubectl` para comunicar-se com o Minikube. Para saber qual Ã© o endereÃ§o IP dessa mÃ¡quina virtual, pode-se executar:

```
minikube ip
```

&nbsp;
O endereÃ§o apresentado Ã© que deve ser utilizado para comunicaÃ§Ã£o com o k8s.

##### Acessando a mÃ¡quina do Minikube via SSH

Para acessar a mÃ¡quina virtual criada pelo Minikube, pode-se executar:

```
minikube ssh
```

&nbsp;

##### Dashboard do Minikube

O Minikube vem com um _dashboard_ _web_ interessante para que o usuÃ¡rio iniciante observe como funcionam os _workloads_ sobre o k8s. Para habilitÃ¡-lo, o usuÃ¡rio pode digitar:

```
minikube dashboard
```

&nbsp;

##### Logs do Minikube

Os _logs_ do Minikube podem ser acessados atravÃ©s do seguinte comando.

```
minikube logs
```

&nbsp;

##### Remover o cluster

```
minikube delete
```

&nbsp;
Caso queira remover o cluster e todos os arquivos referente a ele, utilize o parametro _--purge_, conforme abaixo:

```
minikube delete --purge
```

&nbsp;

#### Kind

O Kind (_Kubernetes in Docker_) Ã© outra alternativa para executar o Kubernetes num ambiente local para testes e aprendizado, mas nÃ£o Ã© recomendado para uso em produÃ§Ã£o.

##### InstalaÃ§Ã£o no GNU/Linux

Para fazer a instalaÃ§Ã£o no GNU/Linux, execute os seguintes comandos.

```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64

chmod +x ./kind

sudo mv ./kind /usr/local/bin/kind
```

&nbsp;

##### InstalaÃ§Ã£o no MacOS

Para fazer a instalaÃ§Ã£o no MacOS, execute o seguinte comando.

```
sudo brew install kind
```

&nbsp;
ou

```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-darwin-amd64
chmod +x ./kind
mv ./kind /usr/bin/kind
```

&nbsp;

##### InstalaÃ§Ã£o no Windows

Para fazer a instalaÃ§Ã£o no Windows, execute os seguintes comandos.

```
curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64

Move-Item .\kind-windows-amd64.exe c:\kind.exe
```

&nbsp;

###### InstalaÃ§Ã£o no Windows via Chocolatey

Execute o seguinte comando para instalar o Kind no Windows usando o Chocolatey.

```
choco install kind
```

&nbsp;

##### Criando um cluster com o Kind

ApÃ³s realizar a instalaÃ§Ã£o do Kind, vamos iniciar o nosso cluster.

```
kind create cluster

Creating cluster "kind" ...
 âœ“ Ensuring node image (kindest/node:v1.24.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? ğŸ˜…  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

```

&nbsp;
Ã‰ possÃ­vel criar mais de um cluster e personalizar o seu nome.

```
kind create cluster --name giropops

Creating cluster "giropops" ...
 âœ“ Ensuring node image (kindest/node:v1.24.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
Set kubectl context to "kind-giropops"
You can now use your cluster with:

kubectl cluster-info --context kind-giropops

Thanks for using kind! ğŸ˜Š
```

&nbsp;
Para visualizar os seus clusters utilizando o kind, execute o comando a seguir.

```
kind get clusters
```

&nbsp;
Liste os nodes do cluster.

```
kubectl get nodes
```

&nbsp;

##### Criando um cluster com mÃºltiplos nÃ³s locais com o Kind

Ã‰ possÃ­vel para essa aula incluir mÃºltiplos nÃ³s na estrutura do Kind, que foi mencionado anteriormente.

Execute o comando a seguir para selecionar e remover todos os clusters locais criados no Kind.

```
kind delete clusters $(kind get clusters)

Deleted clusters: ["giropops" "kind"]
```

&nbsp;
Crie um arquivo de configuraÃ§Ã£o para definir quantos e o tipo de nÃ³s no cluster que vocÃª deseja. No exemplo a seguir, serÃ¡ criado o arquivo de configuraÃ§Ã£o `kind-3nodes.yaml` para especificar um cluster com 1 nÃ³ control-plane (que executarÃ¡ o control plane) e 2 workers.

```
cat << EOF > $HOME/kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
```

&nbsp;
Agora vamos criar um cluster chamado `kind-multinodes` utilizando as especificaÃ§Ãµes definidas no arquivo `kind-3nodes.yaml`.

```
kind create cluster --name kind-multinodes --config $HOME/kind-3nodes.yaml

Creating cluster "kind-multinodes" ...
 âœ“ Ensuring node image (kindest/node:v1.24.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦ ğŸ“¦ ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
 âœ“ Joining worker nodes ğŸšœ
Set kubectl context to "kind-kind-multinodes"
You can now use your cluster with:

kubectl cluster-info --context kind-kind-multinodes

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community ğŸ™‚
```

&nbsp;
Valide a criaÃ§Ã£o do cluster com o comando a seguir.

```
kubectl get nodes
```

&nbsp;
Mais informaÃ§Ãµes sobre o Kind estÃ£o disponÃ­veis em: https://kind.sigs.k8s.io

&nbsp;

### Primeiros passos no k8s

&nbsp;

##### Verificando os namespaces e pods

O k8s organiza tudo dentro de _namespaces_. Por meio deles, podem ser realizadas limitaÃ§Ãµes de seguranÃ§a e de recursos dentro do _cluster_, tais como _pods_, _replication controllers_ e diversos outros. Para visualizar os _namespaces_ disponÃ­veis no _cluster_, digite:

```
kubectl get namespaces
```

&nbsp;
Vamos listar os _pods_ do _namespace_ **kube-system** utilizando o comando a seguir.

```
kubectl get pod -n kube-system
```

&nbsp;
SerÃ¡ que hÃ¡ algum _pod_ escondido em algum _namespace_? Ã‰ possÃ­vel listar todos os _pods_ de todos os _namespaces_ com o comando a seguir.

```
kubectl get pods -A
```

&nbsp;
HÃ¡ a possibilidade ainda, de utilizar o comando com a opÃ§Ã£o `-o wide`, que disponibiliza maiores informaÃ§Ãµes sobre o recurso, inclusive em qual nÃ³ o _pod_ estÃ¡ sendo executado. Exemplo:

```
kubectl get pods -A -o wide
```

&nbsp;

##### Executando nosso primeiro pod no k8s

Iremos iniciar o nosso primeiro _pod_ no k8s. Para isso, executaremos o comando a seguir.

```
kubectl run nginx --image nginx

pod/nginx created
```

&nbsp;
Listando os _pods_ com `kubectl get pods`, obteremos a seguinte saÃ­da.

```
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          66s
```

&nbsp;
Vamos agora remover o nosso _pod_ com o seguinte comando.

```
kubectl delete pod nginx
```

&nbsp;
A saÃ­da deve ser algo como:

```
pod "nginx" deleted
```

&nbsp;

##### Executando nosso primeiro pod no k8s

Uma outra forma de criar um pod ou qualquer outro objeto no Kubernetes Ã© atravÃ©s da utilizaÃ§Ã¢o de uma arquivo manifesto, que Ã© uma arquivo em formato YAML onde vocÃª passa todas as definiÃ§Ãµes do seu objeto. Mas pra frente vamos falar muito mais sobre como construir arquivos manifesto, mas agora eu quero que vocÃª conheÃ§a a opÃ§Ã£o `--dry-run` do `kubectl`, pos com ele podemos simular a criaÃ§Ã£o de um resource e ainda ter um manifesto criado automaticamente.

Exemplos:

Para a criaÃ§Ã£o do template de um _pod_:

```
kubectl run meu-nginx --image nginx --dry-run=client -o yaml > pod-template.yaml
```

&nbsp;
Aqui estamos utilizando ainda o parametro '-o', utilizando para modificar a saÃ­da para o formato YAML.

Para a criaÃ§Ã£o do _template_ de um _deployment_:

Com o arquivo gerado em mÃ£os, agora vocÃª consegue criar um pod utilizando o manifesto que criamos da seguinte forma:

```
kubectl apply -f pod-template.yaml
```

NÃ£o se preocupe por enquanto com o parametro 'apply', nÃ³s ainda vamos falar com mais detalhes sobre ele, nesse momento o importante Ã© vocÃª saber que ele Ã© utilizado para criar novos resources atravÃ©s de arquivos manifestos.

&nbsp;

#### Expondo o pod e criando um Service

Dispositivos fora do _cluster_, por padrÃ£o, nÃ£o conseguem acessar os _pods_ criados, como Ã© comum em outros sistemas de contÃªineres. Para expor um _pod_, execute o comando a seguir.

```
kubectl expose pod nginx
```

SerÃ¡ apresentada a seguinte mensagem de erro:

```
error: couldn't find port via --port flag or introspection
See 'kubectl expose -h' for help and examples
```

O erro ocorre devido ao fato do k8s nÃ£o saber qual Ã© a porta de destino do contÃªiner que deve ser exposta (no caso, a 80/TCP). Para configurÃ¡-la, vamos primeiramente remover o nosso _pod_ antigo:

```
kubectl delete -f pod-template.yaml
```

Agora vamos executar novamente o comando para a criaÃ§Ã£o do pod utilizando o parametro 'dry-run', porÃ©m agora vamos adicionar o parametro '--port' para dizer qual a porta que o container estÃ¡ escutando, lembrando que estamos utilizando o nginx nesse exemplo, um webserver que escuta por padrÃ£o na porta 80.

```
kubectl run meu-nginx --image nginx --port 80 --dry-run=client -o yaml > pod-template.yaml
kubectl create -f pod-template.yaml
```

Liste os pods.

```
kubectl get pods

NAME    READY   STATUS    RESTARTS   AGE
meu-nginx   1/1     Running   0          32s
```

O comando a seguir cria um objeto do k8s chamado de _Service_, que Ã© utilizado justamente para expor _pods_ para acesso externo.

```
kubectl expose pod meu-nginx
```

Podemos listar todos os _services_ com o comando a seguir.

```
kubectl get services

NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   8d
nginx        ClusterIP   10.105.41.192   <none>        80/TCP    2m30s
```

Como Ã© possÃ­vel observar, hÃ¡ dois _services_ no nosso _cluster_: o primeiro Ã© para uso do prÃ³prio k8s, enquanto o segundo foi o quÃª acabamos de criar.

&nbsp;

#### Limpando tudo e indo para casa

Para mostrar todos os recursos recÃ©m criados, pode-se utilizar uma das seguintes opÃ§Ãµes a seguir.

```
kubectl get all

kubectl get pod,service

kubectl get pod,svc
```

Note que o k8s nos disponibiliza algumas abreviaÃ§Ãµes de seus recursos. Com o tempo vocÃª irÃ¡ se familiar com elas. Para apagar os recursos criados, vocÃª pode executar os seguintes comandos.

```
kubectl delete -f pod-template.yaml
kubectl delete service nginx
```

Liste novamente os recursos para ver se os mesmos ainda estÃ£o presentes.

&nbsp;

1cp
3wp
meu-primeiro-cluster.yaml

deploy o pod

**DaemonSet and ReplicaSet** NÃ£o Ã© possÃ­vel utilizar o kubectl create para DaemonSet and ReplicaSet

```
minikube start --driver=virtualbox --nodes=3 --cpus 2 --memory 2200 disk 20000
minikube start --driver=virtualbox --nodes 1 --memory 4096 --cpus 2
minikube delete --purge
minikube addons list
minikube addons enable dashboard
minikube addons enable ingress
minikube addons enable metrics-server
```

## Time Saving Aliases for k8s certification exams

```
## General Aliases
alias k="kubectl"
alias ks="kubectl -n kube-system"
alias kdesc="kubectl describe"

## Create Resources
alias kcf="kubectl create -f"
alias kaf="kubectl apply -f"

## Get Resources
alias kgn="kubectl get nodes"
alias kgp="kubectl get pods"
alias kgpa="kubectl get pods --all-namespaces"
alias kgs="kubectl get services"
alias kgd="kubectl get deployments"

## Delete Resources
alias kd="kubectl delete"
alias kdp="kubectl delete pods"
alias kds="kubectl delete services"
alias kdd="kubectl delete deployments"
alias kdn="kubectl delete namespaces"
```

## 1.27 - Chill Vibes - 2023 - 60 melhorias

- The k8s.gcr.io image registry has been frozen (moved to registry.k8s.io)
- SeccompDefault has graduated to stable
- Node log access is now possible via the k8s API
- Mutable scheduling directives for jobs have graduated to GA, while mutable pod scheduling directives have gone to beta
- DownwardAPIHugePages has graduated to stable
- Pod scheduling Readiness has gone to beta
- ReadWriteOncePod PersistentVolume Access mode has also gone to beta
- We now faster selinux volume relabeling using mounts
- Robust volumemanager reconstruction has gone to beta

## 1.28

## 1.29

## 1.30

## 1.31

## Tips and Tricks
```~/.vimrc
set expandtab
set tabstop=2
set shiftwidth=2
``` 

```Log locations to check:
/var/log/pods
/var/log/containers
crictl ps + crictl logs
docker ps + docker logs (in case when Docker is used)
kubelet logs: /var/log/syslog or journalctl
```

```tmux
## debian and ubuntu
apt install tmux

## macos
brew install tmux

## How to use
tmux

tmux new -s <session_name>

tmux new -s cka
```